---
title: "Dev-Ops for Data Scientists"
subtitle: 'Workshops for Ukraine'
format:
  revealjs:
    slide-number: true
    footer: "<https://github.com/Rikagx/devops-workshop-for-ukraine>"
    preview-links: auto
    incremental: true
    theme:
    - default
    - styles.scss
    width: 1600
    height: 920
knitr:
  opts_chunk:
    echo: true
execute:
  eval: false
resource_files:
- styles.scss
---

## dev-ops is...?

<br>

::: {style="font-size: 2em; color: blue;"}
“... a set of [cultural norms]{.underline}, [practices, and supporting tooling]{.underline} to help make the process of [developing and deploying software]{.underline} [smoother and lower risk]{.underline}."
:::

::: footer
Definition credit: Alex Gold, https://www.do4ds.com
:::

::: notes
Let's get into it. We're at a DevOps workshop - so what is it. This is a best effort attempt but its still a squishy broad buzz word that's defined differently across organization.
:::

##  {auto-animate="true"}

::: {style="margin-top: 200px; font-size: 3em; color: blue;"}
Developing and Deploying Software
:::

## Waterfall Model of Software Development

![](assets/images/02/waterfallsldc.png)

::: notes
Devops became popular in the 2007-2009, across social media, twitter tag #devops was popular, online communities and became more organized through conferences, workshops, and grassroots attempts at implementation at diff companies.

old-school model from the 1970's; each part has to be completed before you go on to the next one, very rigid and linear

can't move forward until you finish each step and then you can't go back if there's a big introduced

Benefits - there's clear structure, fixed costs, process can be replicated Costs - long delivery times, limited innovation, limited flexibility
:::

## Where the waterfall model fails

-   **The time gap**: A developer may work on code that takes days, weeks, or even months to go into production.

-   **The personnel gap**: Developers write code, ops engineers deploy it.

-   **The tools gap**: Developers may be using a stack like Nginx, SQLite, and OS X, while the production deploy uses Apache, MySQL, and Linux.

::: footer
Credit: From the Twelve-Factor App
:::

## Agile Model

![](assets/images/02/mobius.png)

::: notes
teams work in a cycle of planning, executing, and evaluating, iterating as they go.

continuous feedback loops, more communication between teams, adaptability to changes
:::

##  {auto-animate="true"}

::: {style="margin-top: 200px; font-size: 3em; color: blue;"}
Smoother and lower risk
:::

## Problems dev-ops tries to solve

![](assets/images/02/PROBLEMS.png)

::: notes
You want to make the release process as fast as possible but you also want it to be tested and free of bugs. And this introduces a natural conflict between speed and stability

You have the same goals of speed and accuracy - but this process of continuous integration, of testing, of automating processes so that your users can get their hands on the app - didnt exist.

One problem is that the entire process is siloed between the creation of the code and the deployment of your app. So your developers finish their code and then throw it over the fence to ops. Maybe they then throw it to security or to QA. But there was no formal alliance in place or processes on how the teams work together. Because each team is seemingly working on one part - the code vs. the deploy - there's also technical knowledge siloes - each team only knows their bit but not anything else.

So imagine this stock trading app - your developers are creating cool new features so that you can easily buy and sell stock, maybe even see data on whats happening in the market. But they're not necessarily thinking about how the app is secured or if its compliant with federal regulations or if user data is protected. Obvi this is a worst case scenario - but you get the picture.

So as the app is built it keeps getting thrown back and forth between the two teams and leads to one of the major problems that devops is trying to fix - really slow and manual bureaucratic release process.

On the technical side, a lot of the actual work is done manually. So you can imagine one team running tests manually in one environment - maybe the environment itself is created manually, or someone manually sizing up or fixing a server. This takes time, you need to get approval from people, and its not easily reproduced or really documented anywhere. Also, there is a lot of room for error and if things break its not easy to roll back bugs.

Problem becomes how do you automate this release process, make it streamlined, less error prone, improve how all these teams integrate and operate, and at the end of the day make the process of getting the app into the hands of your users much faster.
:::

##  {auto-animate="true"}

::: {style="margin-top: 200px; font-size: 3em; color: blue;"}
Cultural Norms
:::

## Instilling a sense of ...

-   collaboration
-   transparency
-   continuous feedback
-   shared responsibility
-   and most importantly...

![](assets/images/02/communicate.jpeg){fig-align="right"}

::: notes
toolss for cultural change - code review - cross team blameless retros - project kickoffs - everyone owns the process - balance between security and operations - iterative building, making mistakes early and often - identify sources of resistance - get leadership buy-in - design for autonomy - measure output not compliance
:::

## 💬 Discussion

::: {style="margin-top: 200px; font-size: 2em; color: blue;"}
What's the dev-ops culture like where you work?
:::

##  {auto-animate="true"}

::: {style="margin-top: 200px; font-size: 3em; color: blue;"}
Practices and supporting tooling
:::

## Proliferation of tools

![](assets/images/02/ecosystem.png)

::: notes
proliferation of tools, source code management, repo management, build tools, data management, deployment, container services, config management, monitoring, cloud services
:::

## Best Practices from the 12factor App

::: nonincremental
1.  **Codebase** - One codebase tracked in revision control, many deploys
2.  **Dependencies** - Explicitly declare and isolate dependencies
3.  **Config** - Store config in the environment
4.  **Backing services** - Treat backing services as attached resources
5.  **Build, release, run** - Strictly separate build and run stages
6.  **Processes** - Execute the app as one or more stateless processes
7.  *Port binding - Export services via port binding*
8.  *Concurrency - Scale out via the process model*
9.  *Disposability - Maximize robustness with fast startup and graceful shutdown*
10. **Dev/prod/ parity** - Keep development, staging, and production as similar as possible
11. **Logs** - Treat logs as event streams
12. **Admin processes** - Run admin/management tasks as one-off processes
:::

::: notes
2011 by Adam Wiggins - founder of heroku. The 12 Factor App is a set of principles that describes a way of making software that, when followed, enables companies to create code that can be released reliably, scaled quickly, and maintained in a consistent and predictable manner.

Nice baseline for building portable and resilent applications
:::

## Codebase

<br>

There should be exactly one codebase for a deployed service with the codebase being used for many deployments.

![](assets/images/02/codebase.png)

::: footer
Credit: An illustrated guide to 12 Factor Apps by Bob Reselman
:::

## Dependencies

<br>

All dependencies should be declared, with no implicit reliance on system tools or libraries.

![](assets/images/02/dependencies.png)

::: footer
Credit: An illustrated guide to 12 Factor Apps by Bob Reselman
:::

## Config

::: nonincremental
-   Configuration that varies between deployments should be stored in the environment.
-   A litmus test for whether an app has all config correctly factored out of the code is whether the codebase could be made open source at any moment, without compromising any credentials.
:::

![](assets/images/02/config.png)

::: footer
Credit: An illustrated guide to 12 Factor Apps by Bob Reselman
:::

## Backing services

<br>

All backing services are treated as attached resources and attached and detached by the execution environment. ![](assets/images/02/backingsvcs.png){width="498"}

::: footer
Credit: An illustrated guide to 12 Factor Apps by Bob Reselman
:::

## Build, release, run

<br>

![](assets/images/02/buildreleaserun.png)

::: footer
Credit: An illustrated guide to 12 Factor Apps by Bob Reselman
:::

::: notes
-   The **Build** stage is where code is retrieved from the source code management system and built/compiled into artifacts.
-   After the code is built, configuration settings are applied in the **Release** stage.
-   Then, in the **Run** stage, a runtime environment is provisioned via scripts. The application and its dependencies are deployed into the newly provisioned runtime environment.
-   The key to **Build, Release, and Run** is that the process is completely ephemeral. Should anything in the pipeline be destroyed, all artifacts and environments can be recreated from scratch using source code repository.
:::

## Processes

<br> Applications should be deployed as one or more stateless processes with persisted data stored on a backing service.

![](assets/images/02/process.png)

::: footer
Credit: An illustrated guide to 12 Factor Apps by Bob Reselman
:::

::: notes
This means that no single process keeps track of the state of another process and that no process keeps track of information such as session or workflow status. A stateless process makes scaling easier.
Discuss shiny and other types of applications, caching, etc
:::

## Dev/Prod parity

<br>

+---------------------------------+-------------------------------------------------------------+------------------------------+
| Development                     | Testing                                                     | Production                   |
+=================================+=============================================================+==============================+
| -   Exploratory                 | -   as similar to prod as possible                          | -   automatic CD             |
|                                 |                                                             |                              |
| -   Often local machine         | -   unit & integration testing                              | -   isolated from dev & test |
|                                 |                                                             |                              |
| -   Access to R/Python Packages | -   data validation                                         | -   created with code        |
|                                 |                                                             |                              |
|                                 | -   "sandbox" with data that's as close to real as possible |                              |
+---------------------------------+-------------------------------------------------------------+------------------------------+

## Logs

<br>

Applications should produce logs as event streams and leave the execution environment to aggregate. ![](assets/images/02/logs.png)

::: footer
Credit: An illustrated guide to 12 Factor Apps by Bob Reselman
:::

## Admin Processes

<br>

::: nonincremental
-   Any needed admin tasks should be kept in source control and packaged with the application.
-   Admin processes are first-class citizens in the software development lifecycle and need to be treated as such.
:::

![](assets/images/02/admin-processes.png)

::: footer
Credit: An illustrated guide to 12 Factor Apps by Bob Reselman
:::

## A simplified framework for tools

![](assets/images/02/bestpractice.jpg)

##
::: nonincremental

1. [Collaboration & Version Control]{.mark}
2. Environment Management & Reproducibility
3. Continuous Integration & Deployment
4. Automation
5. Containers & Orchestration
6. Observability & Monitoring

:::

## Version Control & Collaboration

![](assets/images/02/gitworkflow.png)

::: footer
Artwork by @allison_horst
:::

::: notes
A complete history of every file, which enables you to go back to previous versions to analyze the source of bugs and fix problems in older versions.

Branching and Merging: The ability to work on independent streams of changes, which allows you to merge that work back together and verify that your changes conflict.

Traceability: The ability to trace each change with a message describing the purpose and intent of the change and connect it to project management and bug tracking software.

Centralized single source of truth

In addition to the action of promoting your code - its also important to have processes in place for how the code integration process occurs - includes humans coming together and making some decisions -

e.g code reviews, process for your team, how to name things, pull requests, merging, feature branching, automatic tests
:::

##
::: nonincremental

1. Collaboration & Version Control
2. [Environment Management & Reproducibility]{.mark}
3. Continuous Integration & Deployment
4. Automation
5. Containers & Orchestration
6. Observability & Monitoring

:::


## 💬 Environment Management & Reproducibility

::: {style="margin-top: 200px; font-size: 1.5em; color: blue;"}
What are the layers that need to be reproduced across your dev, test, and prod environments?

<br>

What's your most difficult reproducibility challenge?
:::

## Layers of reproducibility

![](assets/images/02/layerstoreproduce.png)

::: notes
-   code - scripts, configs, applications
-   Packages
-   System - r and python depend on underlying system software - for example, spatial analysis packages, or anything that requires Java - rJava
-   OS
-   Hardware - processors Intel chip, silicon chip
:::

## Packages vs. Libraries vs. Repositories

![](assets/images/02/cupcakes.png)

Think of your data science workbench as a kitchen:

-   The **repository** is the grocery store, a central place where everyone gets their packages.
-   The **library** is the pantry, where you keep your own **private** set of packages.
-   **Installation** is the shopping trip to stock your library with **packages** (e.g.ingredients) from the repository.

## Managing Environments

![](assets/images/02/repromap.png)

## Virtual Environments

![](assets/images/02/snapshotrestore.png)

::: panel-tabset
### R

``` r
install.packages("renv")
renv::init()
renv::snapshot()
```

### Python

``` python
python -m venv .venv
source .venv/bin/activate
pip freeze > requirements.txt
```
:::

##
::: nonincremental

1. Collaboration & Version Control
2. Environment Management & Reproducibility
3. [Continuous Integration & Deployment]{.mark}
4. Automation
5. Containers & Orchestration
6. Observability & Monitoring

:::


## Continuous Integration & Delivery

![](assets/images/02/cicd2.png)

::: notes
-   As we discussed before CI/CD is an iterative cycle of small steps to quickly build, test, and deploy your code - this is a critical component of devops.

-   So CI/CD is often said in the same breath - as it makes up a continuous pipeline - but its actually pretty discrete parts so I want to make sure we understand how they differ.

-Continuous integration (CI) - this is where you or anyone who writes code, automatically builds, tests, and commits code changes into a shared repository; This is usually triggered by an automatic process where the code is built, tested and then either passes or fails. This step is focused on improving the actual build or application as quickly as possible.

Different types of tests - from unit tests to integration tests to regression tests.

-Continuous delivery (CD) and deployment are less focused on the build but on the actual installation and distribution of your build. This includes an automated process to deploy across different environments - from development to testing and finally to production. Delivery is the process for the final release - that "push of a button step" to get to prod.
:::

## From CI to delivery & deployment {auto-animate="true"}

![](assets/images/02/CICDauto.png)

::: footer
Image Credit: https://www.atlassian.com/continuous-delivery/principles/continuous-integration-vs-delivery-vs-deployment
:::

::: notes
continuous integration is part of both continuous delivery and continuous deployment. And continuous deployment is like continuous delivery, except that releases happen automatically

Both Continuous Delivery and Continuous Deployment aim to increase the speed, frequency, and reliability of software releases, but they differ in the level of automation and control over the deployment process. The choice between the two approaches depends on the team’s needs, the nature of the software, and the organisation’s risk tolerance.
:::

## Defining Production

<br>

When other people are using your:

-   data
-   app
-   api
-   dashboard
-   model


## Getting to Production

<br>

You've written a kickass app! Now what?

## Production Quality

<br>

-   **Correct**: the data product works as expected
-   **Available**: unplanned outages are rare or nonexistent
-   **Safe**: data, functionality, and code are all kept safe from unauthorized users or unintended alteration
-   **Snappy**: fast response times, ability to predict needed capacity for expanded traffic
-   **Sturdy**: design and test to minimize the likelihood that changes will break things

## 💬 Discussion

-   How are we presenting our code?

-   What is the data architecture?

-   Where is it being deployed?

-   Is it secure and accessible?

-   Does it scale?

## Think about data auth early!

![](assets/images/03/whose-creds.png)

::: footer
Illustration by Alex Gold www.do4ds.com
:::

## Choosing the right presentation layer

<br>

*Presentation Layer* 🖼️ – what the end users of the app directly interact with. It’s the displays, buttons, and functionality the user experiences.

<br>

*Processing Layer* ⚒️ – the processing that happens as a result of user interactions. Sometimes, it is called the *business logic*.

<br>

*Data Layer* 🛢️ – how and where the app stores and retrieves data.

## Presentation Layer

![](assets/images/03/presentation-layer.png)

::: footer
Illustration by Alex Gold www.do4ds.com
:::

## What's happening with the data?

-   Can you remove the data processing and storage from the presentation?

-   Can you pre-calculate anything?

-   Can you reduce data granularity?

-   When is data pulled in and refreshed?

## Data Storage

| Location      | Use case                                                                                                                                                                                                                                |
|---------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| With the code | Data is updated as often or less often than the app code and doesn’t need to be shared across projects                                                                                                                                  |
| Database      | Gold standard for data storage and access                                                                                                                                                                                               |
| Pins          | Lightweight datasets, ephemeral data, models                                                                                                                                                                                            |
| Blob Storage  | Gold standard for storing large amounts of unstructured data                                                                                                                                                                            |
| In the server | Typically the method of last resort for unstructured data as it requires SSH access to the server for setup and often requires code changes between development and deployment, however for very large files, it may be the only option |
| API           | Gold standard for data that requires long-running business logic, like training an ML model                                                                                                                                             |


## Importance of Testing

![](assets/images/02/bsod.jpeg)

## Types of Testing {.smaller}

+--------------------------+--------------------------------------------------------------------------------+
| Type                     | What it tests                                                                  |
+==========================+================================================================================+
| Unit                     | Do (small) individual bits of code work as they should?                        |
+--------------------------+--------------------------------------------------------------------------------+
| Integration              | Do all the pieces work together? E.g. (model + API + app +db)                  |
+--------------------------+--------------------------------------------------------------------------------+
| Functional               | Do outputs follow business logic and rules?                                    |
|                          |                                                                                |
|                          | Does the data look as it should? Are models outputting correct information     |
+--------------------------+--------------------------------------------------------------------------------+
| End-to-end               | Replicates and tests behavior from beginning to end including user interaction |
+--------------------------+--------------------------------------------------------------------------------+
| User Acceptance          | Looking for bugs, inconsistencies, errors, from end-user perspective           |
+--------------------------+--------------------------------------------------------------------------------+
| Quality Assurance        | Looking for bugs, inconsistencies, errors, from risk, product perspective      |
+--------------------------+--------------------------------------------------------------------------------+
| Performance/Optimization | Are applications reliable, fast, scalable, and responsive                      |
+--------------------------+--------------------------------------------------------------------------------+
| Smoke test/canary test   | Does the application break anything?                                           |
+--------------------------+--------------------------------------------------------------------------------+

## Github Actions for CI/CD

::: {layout-nrow="2"}
![](assets/images/02/ghdeploy.jpg){fig-align="left" width="1000"}

![](assets/images/02/ghjobssteps.png){fig-align="left" width="1000"}
:::

## 🔍 GHA Syntax

``` yaml
---

name: learn-github-actions

# Specifies the trigger for this workflow. This example uses the `push` event, so a workflow run is triggered every time someone pushes a change to the repository or merges a pull request.
on: [push]

# Groups together all the jobs that run in the `learn-github-actions` workflow
jobs:

# Defines a job named `check-bats-version`. Bats refers to Bash Automated Testing.
  check-bats-version:

# Configures the job to run on the latest version of an Ubuntu Linux runner. This means that the job will execute on a fresh virtual machine hosted by GitHub.
    runs-on: ubuntu-latest

# Groups together all the steps that run in the `check-bats-version` job. Each item nested under this section is a separate action or shell script.
    steps:

# The `uses` keyword specifies that this step will run `v4` of the `actions/checkout` action. This is an action that checks out your repository onto the runner, allowing you to run scripts or other actions against your code (such as build and test tools). You should use the checkout action any time your workflow will use the repository's code.
      - uses: actions/checkout@v4

# This step uses the `actions/setup-node@v4` action to install the specified version of the Node.js. (This example uses version 20.)
      - uses: actions/setup-node@v4
        with:
          node-version: '20'

# The `run` keyword tells the job to execute a command on the runner. In this case, you are using `npm` to install the `bats` software testing package.
      - run: npm install -g bats

# Finally, you'll run the `bats` command with a parameter that outputs the software version.
      - run: bats -v
---
```

## Power of YAML

-   YAML Ain't Markup Language

-   communication of data between people and computers

-   human friendly

-   configures files across many execution environments

## YAML Syntax

```
EmpRecord:
  emp01:
    name: Michael
    job: Manager
    skills:
      - Improv
      - Public speaking
      - People management
  emp02:
    name: Dwight
    job: Assistant to the Manager
    skills:
      - Martial Arts
      - Beets
      - Sales
```

-   whitespace indentation denotes structure & hierarchy

-   Colons separate keys and their values

-   Dashes are used to denote a list

## Open source ecosystem of actions

-   [Github Official Actions](https://www.github.com/actions)
-   [RLib Actions](https://www.github.com/r-lib/actions)

## 🔍 Publish Shiny App

``` yaml
---
name: publish
on:
  push:
    branches: [main]
  pull_request:
    branches: [main]

jobs:
  shiny-deploy:
    runs-on: ubuntu-latest
    env:
      GITHUB_PAT: ${{ secrets.GITHUB_TOKEN }}
    steps:
      - uses: actions/checkout@v4

      - uses: r-lib/actions/setup-pandoc@v2

      - uses: r-lib/actions/setup-r@v2
        with:
          use-public-rspm: true
          r-version: renv

      - uses: r-lib/actions/setup-renv@v2

      - name: Install rsconnect
        run: install.packages("rsconnect")
        shell: Rscript {0}

      - name: Authorize and deploy app
        env:
          # Provide your app name, account name, and server to be deployed below
          APPNAME: your-app-name
          ACCOUNT: your-account-name
          SERVER: shinyapps.io # server to deploy
        run: |
          rsconnect::setAccountInfo("${{ secrets.RSCONNECT_USER }}", "${{ secrets.RSCONNECT_TOKEN }}", "${{ secrets.RSCONNECT_SECRET }}")
          rsconnect::deployApp(appName = "${{ env.APPNAME }}", account = "${{ env.ACCOUNT }}", server = "${{ env.SERVER }}")
---
```

::: notes
Workflows can include tests, markdown renders, shell scripts, cron jobs, or deployments. They can be as simple or as complicated as you need. Open-source community provides a ton of examples of actions.
:::


## 🔍 Test Coverage

``` yaml
---

on:
  push:
    branches: [main, master]
  pull_request:
    branches: [main, master]

name: test-coverage.yaml

permissions: read-all

jobs:
  test-coverage:
    runs-on: ubuntu-latest
    env:
      GITHUB_PAT: ${{ secrets.GITHUB_TOKEN }}

    steps:
      - uses: actions/checkout@v4

      - uses: r-lib/actions/setup-r@v2
        with:
          use-public-rspm: true

      - uses: r-lib/actions/setup-r-dependencies@v2
        with:
          extra-packages: any::covr, any::xml2
          needs: coverage

      - name: Test coverage
        run: |
          cov <- covr::package_coverage(
            quiet = FALSE,
            clean = FALSE,
            install_path = file.path(normalizePath(Sys.getenv("RUNNER_TEMP"), winslash = "/"), "package")
          )
          covr::to_cobertura(cov)
        shell: Rscript {0}

      - uses: codecov/codecov-action@v4
        with:
          fail_ci_if_error: ${{ github.event_name != 'pull_request' && true || false }}
          file: ./cobertura.xml
          plugin: noop
          disable_search: true
          token: ${{ secrets.CODECOV_TOKEN }}

      - name: Show testthat output
        if: always()
        run: |
          ## --------------------------------------------------------------------
          find '${{ runner.temp }}/package' -name 'testthat.Rout*' -exec cat '{}' \; || true
        shell: bash

      - name: Upload test results
        if: failure()
        uses: actions/upload-artifact@v4
        with:
          name: coverage-test-failures
          path: ${{ runner.temp }}/package
---
```

## 🔍 Code Linter

``` yaml
---

on:
  push:
    branches: [main, master]
  pull_request:
    branches: [main, master]

name: lint.yaml

permissions: read-all

jobs:
  lint:
    runs-on: ubuntu-latest
    env:
      GITHUB_PAT: ${{ secrets.GITHUB_TOKEN }}
    steps:
      - uses: actions/checkout@v4

      - uses: r-lib/actions/setup-r@v2
        with:
          use-public-rspm: true

      - uses: r-lib/actions/setup-r-dependencies@v2
        with:
          extra-packages: any::lintr, local::.
          needs: lint

      - name: Lint
        run: lintr::lint_package()
        shell: Rscript {0}
        env:
          LINTR_ERROR_ON_LINT: true
---
```
## Where to deploy our app?

![](assets/images/03/slides.png)

-   cloud container deployment + registry

-   build your own

-   platform as a service

-   Cloud platform all in one

## Things to consider when choosing a deployment framework

-   cost

-   how many apps to deploy? what kinds of apps?

-   who are users? do they need to login?

-   how secure does the data need to be? (and the server it resides in)

-   compute resources

-   integration with git and ci/cd

## Posit Connect

![](assets/images/03/Posit%20Team%20Concept%20Map.jpg)

## Connect Cloud


![](assets/images/02/posit-connect-cloud.jpg)


##
::: nonincremental

1. Collaboration & Version Control
2. Environment Management & Reproducibility
3. Continuous Integration & Deployment
4. [Automation]{.mark}
5. Containers & Orchestration
6. Observability & Monitoring

:::


## Automating Tasks

<br> <iframe src="https://giphy.com/embed/7XsFGzfP6WmC4" width="480" height="366" frameBorder="0" class="giphy-embed" allowFullScreen></iframe>

-   Provisioning Infrastructure
-   Testing & Monitoring
-   Integration & deployment

## Tools for Automation

![](assets/images/02/Visual%20Table.jpg)

## 💬 Discussion

::: {style="margin-top: 200px; font-size: 1.5em; color: blue;"}
Why should we automate the above tasks?

<br>

When should we NOT automate a task or process?
:::

## 🔍 Pulumi Example

![](assets/images/02/pulumiaws.png)

```{bash}
# Create directory and project folder
mkdir my-virtual-machine && cd my-virtual-machine && pulumi new vm-aws-python

# Create and configure a new stack
pulumi stack init dev
pulumi config set aws:region us-east-2

# Preview and run the deployment
pulumi up

# Remove the app
pulumi destroy
pulumi stack rm

```

https://github.com/pulumi/examples/tree/master/aws-py-resources

## Pre-commit hooks

::: nonincremental
-   run before every commit
-   configured in `.pre-commit-config.yaml`
-   when should we NOT use a pre-commit hook?
-   which tasks are useful to have in a pre-commit hook?
:::

::: {.callout-tip icon="false"}
Black: The uncompromising Python code formatter

*By using Black, you agree to cede control over minutiae of hand-formatting. In return, Black gives you speed, determinism, and freedom from pycodestyle nagging about formatting. You will save time and mental energy for more important matters.*
:::

## 🔍 Before Black

``` python
from seven_dwwarfs import Grumpy, Happy, Sleepy, Bashful, Sneezy, Dopey, Doc
x = {  'a':37,'b':42,

'c':927}

x = 123456789.123456789E123456789

if very_long_variable_name is not None and \
 very_long_variable_name.field > 0 or \
 very_long_variable_name.is_debug:
 z = 'hello '+'world'
else:
 world = 'world'
 a = 'hello {}'.format(world)
 f = rf'hello {world}'
if (this
and that): y = 'hello ''world'#FIXME: https://github.com/psf/black/issues/26
class Foo  (     object  ):
  def f    (self   ):
    return       37*-2
  def g(self, x,y=42):
      return y
def f  (   a: List[ int ]) :
  return      37-a[42-u :  y**3]
def very_important_function(template: str,*variables,file: os.PathLike,debug:bool=False,):
    """Applies `variables` to the `template` and writes to `file`."""
    with open(file, "w") as f:
     ...
# fmt: off
custom_formatting = [
    0,  1,  2,
    3,  4,  5,
    6,  7,  8,
]
# fmt: on
regular_formatting = [
    0,  1,  2,
    3,  4,  5,
    6,  7,  8,
]
```

## 🔍 After Black

``` python
from seven_dwwarfs import Grumpy, Happy, Sleepy, Bashful, Sneezy, Dopey, Doc

x = {"a": 37, "b": 42, "c": 927}

x = 123456789.123456789e123456789

if (
    very_long_variable_name is not None
    and very_long_variable_name.field > 0
    or very_long_variable_name.is_debug
):
    z = "hello " + "world"
else:
    world = "world"
    a = "hello {}".format(world)
    f = rf"hello {world}"
if this and that:
    y = "hello " "world"  # FIXME: https://github.com/psf/black/issues/26


class Foo(object):
    def f(self):
        return 37 * -2

    def g(self, x, y=42):
        return y


def f(a: List[int]):
    return 37 - a[42 - u : y**3]


def very_important_function(
    template: str,
    *variables,
    file: os.PathLike,
    debug: bool = False,
):
    """Applies `variables` to the `template` and writes to `file`."""
    with open(file, "w") as f:
        ...


# fmt: off
custom_formatting = [
    0,  1,  2,
    3,  4,  5,
    6,  7,  8,
]
# fmt: on
regular_formatting = [
    0,
    1,
    2,
    3,
    4,
    5,
    6,
    7,
    8,
]
```

## ✏️ Try it out {.smaller transition="slide-in"}

::: {.callout-note icon="false"}
## Build a pre-commit hook

```{bash}
# run in your bash terminal
pip install pre-commit
pip install black
```

Create a file called *.pre-commit.config.yaml* and add the following

```{bash}
repos:
  - repo: https://github.com/psf/black-pre-commit-mirror
    rev: 23.10.1
    hooks:
    - id: black
    language_version: python3.12
```

```{bash}
# run in your bash terminal
pre-commit install
```
:::

##
::: nonincremental

1. Collaboration & Version Control
2. Environment Management & Reproducibility
3. Continuous Integration & Deployment
4. Automation
5. [Containers & Orchestration]{.mark}
6. Observability & Monitoring

:::

## Containers & Orchestration

![](assets/images/02/dockerk8s.png)

-   **Consistency:** ensure that applications run the same way across different environments.
-   **Isolation:** isolate applications and their dependencies, preventing conflicts.
-   **Portability:** run on any system that supports container, reducing "it works on my machine" issues.

## Benefits

-   allows you to package up everything you need to reproduce an environment/application
-   lightweight system without much overhead
-   share containers with colleagues without requiring them to have to set up their own local machines
-   quick testing and debugging
-   allows you to easily version snapshots of your work
-   scaling up with limited local compute
-   Create isolated environments for different experiments.

## Containerization with Docker

-   Isolation of applications inside individual OS-based environments inside virtual machines or physical servers

-   Super lightweight and fast to spin-up (much faster than a VM)

-   Made up of individual layers so its really quick to build

-   Can build isolated applications from their own image

-   Containers are immutable and ephemeral

![](assets/images/02/dockerfile-layers.png)


## How Docker works

![](assets/images/02/lifecycle.png)

::: notes

dockerfile - is a script of instructions for how to build an image

image - everything you need to run an application - all the layers that build the environment, dependencies, libraries, files

container - isolated instance of a running image. you can create, stop, start, restart, containers. When a container is removed/deleted any changes to its state that arent stored in some kind of persistent storage disappear. Called ephemeral container - Think of a container as a snapshot in time of a particular application.

Missing piece - repository - for images, like dockerhub, container registry cloud services, private registries

Build - Run - Push

:::


## Docker in CI

``` yaml
---

jobs:
  build:
    runs-on: ubuntu-latest
    steps:
      -
        name: Checkout
        uses: actions/checkout@v3
      -
        name: Login to Docker Hub
        uses: docker/login-action@v2
        with:
          username: ${{ secrets.DOCKERHUB_USERNAME }}
          password: ${{ secrets.DOCKERHUB_TOKEN }}
      -
        name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v2
      -
        name: Build and push
        uses: docker/build-push-action@v4
        with:
          context: .
          file: ./Dockerfile
          push: true
          tags: ${{ secrets.DOCKERHUB_USERNAME }}/latest
---
```


##
::: nonincremental

1. Collaboration & Version Control
2. Environment Management & Reproducibility
3. Continuous Integration & Deployment
4. Automation
5. Containers & Orchestration
6. [Observability & Monitoring]{.mark}

:::


## Observability & Monitoring

<br>

We want to observe...

-   Operations

-   Correctness

-   Internal state

-   Data Flow & Lineage

-   Errors

::: {.callout-note icon="false"}
*Observability is...a measure of how well you can understand and explain any state your system can get into, no matter how novel or bizarre \[...\] without needing to ship new code.*

— Honeycomb.io
:::

## Logging

-   recording execution of your code to stout or log file
-   like `print` statements but for production
-   useful for long running processes
-   when you can't simply stop the workflow

## What to log

-   functions/jobs/tests have executed correctly or incorrectly
-   error messages
-   inputs and output of a function or job
-   where things have been saved

## Log Levels

<br>

**Debug**: detail on what the code was doing

**Info**: something normal happened in the app

**Warn/Warning**: an unexpected application issue that isn’t fatal

**Error**: an issue that will make an operation not work, but that won’t crash your app.

**Critical**: an error so big that the app itself shuts down.

## 🔍Logging in R

<br>

::: columns
::: {.column width="50%"}
```{r}

library(log4r)

# Configure your logging file
my_logfile = "my_logfile.txt"

# Configure your console and file layout

my_console_appender = console_appender(layout = default_log_layout())
my_file_appender = file_appender(my_logfile, append = TRUE,
                            layout = default_log_layout())

# the log severity you are using
my_logger <- log4r::logger(threshold = "INFO",
                appenders= list(my_console_appender,my_file_appender))

# functions that you will add into your script
log4r_info <- function() {
  log4r::info(my_logger, "Info_message.")
}

log4r_error <- function() {
  log4r::error(my_logger, "Error_message")
}

log4r_debug <- function() {
  log4r::debug(my_logger, "Debug_message")
}

```
:::

::: {.column width="50%"}
```{r}
log4r_debug() # will not trigger log entry because threshold was set to INFO

log4r_info()
#> INFO  [2024-09-01 12:30:05] Info_message.

log4r_error()
#> ERROR [2024-09-01 12:30:05] Error_message

readLines(my_logfile)
#> [1] "INFO  [2024-09-01 12:30:05] Info_message."
#> [2] "ERROR [2024-09-01 12:30:05] Error_message"
```
:::
:::

## 🔍Logging in Python

::: columns
::: {.column width="50%"}
``` python
import logging

# get or create logger
logger = logging.getLogger(__name__)
# set log level
logger.setLevel(logging.WARNING)

# define file handler and set formatter
file_handler = logging.FileHandler('logfile.log')
formatter    = logging.Formatter('%(asctime)s : %(levelname)s : %(name)s : %(message)s')
file_handler.setFormatter(formatter)

# add file handler to logger
logger.addHandler(file_handler)

# Logs
logger.debug('A debug message')
logger.info('An info message')
logger.warning('Something is not right.')
logger.error('A Major error has happened.')
logger.critical('Fatal error. Cannot continue')
```
:::

::: {.column width="50%"}
``` python
logging.debug("debug_message") # will not trigger log entry because threshold was set to INFO

#> 2024-09-01 12:30:05,797 : WARNING : __main__ : Something is not right.
#> 2024-09-01 12:30:05,798 : ERROR : __main__ : A Major error has happened.
#> 2024-09-01 12:30:05,798 : CRITICAL : __main__ : Fatal error. Cannot continue
```
:::
:::


## Thank you so much!

![](assets/images/04/cat.png)
